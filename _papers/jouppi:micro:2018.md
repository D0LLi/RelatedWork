---
ENTRYTYPE: article
abstract: The first-generation tensor processing unit (TPU) runs deep neural network (DNN) inference 15-30 times faster with 30-80 times better energy efficiency
  than contemporary CPUs and GPUs in similar semiconductor technologies. This domain-specific architecture (DSA) is a custom chip that has been deployed
  in Google datacenters since 2015, where it serves billions of people.
added: 2021-09-29
authors:
- Norman Jouppi
- Cliff Young
- Nishant Patil
- David Patterson
doi: 10.1109/MM.2018.032271057
issn: 1937-4143
journal: IEEE Micro
keywords: ''
layout: paper
month: May
number: '3'
pages: 10-19
read: false
readings: []
title: Motivation for and evaluation of the first tensor processing unit
volume: '38'
year: 2018
notes:
- neural network
- MLP
- CNN
- RNN
- tensor
- TensorFlow
- hardware
papers:
- jouppi:isca:2017
---
{% include links.html %}
